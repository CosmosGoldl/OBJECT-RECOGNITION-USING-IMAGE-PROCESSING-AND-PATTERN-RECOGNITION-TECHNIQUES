import cv2
import numpy as np
import onnxruntime as ort
import time
import os
import pyttsx3
from threading import Thread
from queue import Queue

# --- 1. C·∫§U H√åNH ---
USE_CAM = True  # Chuy·ªÉn sang True ƒë·ªÉ s·ª≠ d·ª•ng camera
VIDEO_PATH = "./sample_video.mp4"  # ƒê∆∞·ªùng d·∫´n video m·∫´u (n·∫øu c√≥)

# DLL Paths - Kh√¥ng c·∫ßn thi·∫øt tr√™n macOS
# Ch·ªâ s·ª≠ d·ª•ng tr√™n Windows v·ªõi CUDA
# cuda_bin = r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4\bin"
# cudnn_bin = r"C:\Program Files\NVIDIA\CUDNN\v9.17\bin"
# if os.path.exists(cuda_bin): os.add_dll_directory(cuda_bin)
# if os.path.exists(cudnn_bin): os.add_dll_directory(cudnn_bin)

# --- 2. KH·ªûI T·∫†O B·∫¢NG LUT (GAMMA CORRECTION) ---
# Gamma < 1.0 l√†m s√°ng v√πng t·ªëi m√† kh√¥ng ch√°y v√πng s√°ng. 0.8 l√† m·ª©c t·ªëi ∆∞u.
gamma = 0.8
invGamma = 1.0 / gamma
lut_table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype("uint8")

# --- 3. KH·ªûI T·∫†O MODEL ---
# ∆Øu ti√™n CPU tr√™n macOS, GPU n·∫øu c√≥
providers = ['CPUExecutionProvider']
# N·∫øu c√≥ GPU AMD/Metal tr√™n Mac M1/M2, c√≥ th·ªÉ th√™m 'CoreMLExecutionProvider'
session_coco = None
session_custom = None 

try:
    opt = ort.SessionOptions()
    session_coco = ort.InferenceSession("yolov10s.onnx", providers=providers, sess_options=opt)
    # session_custom = ort.InferenceSession("best.onnx", providers=providers, sess_options=opt)
    print("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng v·ªõi CPU")
except Exception as e:
    print(f"‚ùå L·ªói: {e}"); exit()

class_names_coco = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']
class_names_custom = ['door', 'trash_can']

# --- 4. H·ªÜ TH·ªêNG √ÇM THANH V√Ä T√çNH KHO·∫¢NG C√ÅCH ---

# C·∫•u h√¨nh k√≠ch th∆∞·ªõc trung b√¨nh cho c√°c lo·∫°i v·∫≠t th·ªÉ (ƒë·ªÉ t√≠nh kho·∫£ng c√°ch)
class_avg_sizes = {
    "person": {"width_ratio": 2.5},
    "car": {"width_ratio": 0.37},
    "bicycle": {"width_ratio": 2.3},
    "motorcycle": {"width_ratio": 2.4},
    "bus": {"width_ratio": 0.3},
    "traffic light": {"width_ratio": 2.95},
    "stop sign": {"width_ratio": 2.55},
    "bench": {"width_ratio": 1.6},
    "cat": {"width_ratio": 1.9},
    "dog": {"width_ratio": 1.5},
}

# Bi·∫øn global cho h·ªá th·ªëng √¢m thanh
audio_queue = None
audio_thread = None

def setup_audio_system():
    """Kh·ªüi t·∫°o h·ªá th·ªëng ph√°t √¢m thanh"""
    global audio_queue, audio_thread
    
    def speak(q):
        engine = pyttsx3.init()
        engine.setProperty('rate', 235)  # T·ªëc ƒë·ªô n√≥i
        engine.setProperty('volume', 1.0)  # √Çm l∆∞·ª£ng

        while True:
            if not q.empty():
                label, distance, position = q.get()
                rounded_distance = round(distance * 2) / 2  # L√†m tr√≤n 0.5
                
                # Chuy·ªÉn ƒë·ªïi s·ªë th√†nh chu·ªói (b·ªè .0 n·∫øu l√† s·ªë nguy√™n)
                rounded_distance_str = str(int(rounded_distance)) if rounded_distance.is_integer() else str(rounded_distance)
                
                # Ph√°t √¢m thanh th√¥ng b√°o
                if label in class_avg_sizes:
                    engine.say(f"{label} IS {rounded_distance_str} METERS ON {position}")
                    engine.runAndWait()
                
                # X√≥a queue sau khi ph√°t
                with q.mutex:
                    q.queue.clear()
            else:
                time.sleep(0.1)  # Tr√°nh busy waiting
    
    # T·∫°o queue v√† thread cho audio
    audio_queue = Queue()
    audio_thread = Thread(target=speak, args=(audio_queue,))
    audio_thread.daemon = True  # Thread s·∫Ω d·ª´ng khi main program d·ª´ng
    audio_thread.start()
    
    return audio_queue

def calculate_distance(object_width, frame_width, label):
    """
    T√≠nh kho·∫£ng c√°ch d·ª±a tr√™n ƒë·ªô r·ªông v·∫≠t th·ªÉ trong frame
    """
    # √Åp d·ª•ng h·ªá s·ªë hi·ªáu ch·ªânh cho t·ª´ng lo·∫°i v·∫≠t th·ªÉ
    if label in class_avg_sizes:
        object_width *= class_avg_sizes[label]["width_ratio"]
    
    # C√¥ng th·ª©c t√≠nh kho·∫£ng c√°ch d·ª±a tr√™n FOV 70 ƒë·ªô
    distance = (frame_width * 0.5) / np.tan(np.radians(70 / 2)) / (object_width + 1e-6)
    return round(distance, 2)

def get_position(frame_width, x_center):
    """X√°c ƒë·ªãnh v·ªã tr√≠ c·ªßa v·∫≠t th·ªÉ (LEFT/FORWARD/RIGHT)"""
    if x_center < frame_width // 3:
        return "LEFT"
    elif x_center < 2 * (frame_width // 3):
        return "FORWARD"
    else:
        return "RIGHT"

def blur_person_face(frame, x1, y1, x2, y2):
    """L√†m m·ªù khu√¥n m·∫∑t ng∆∞·ªùi ƒë·ªÉ b·∫£o v·ªá privacy"""
    h = y2 - y1
    # L√†m m·ªù ph·∫ßn ƒë·∫ßu (8% chi·ªÅu cao t·ª´ tr√™n xu·ªëng)
    face_y2 = y1 + int(0.08 * h)
    if face_y2 > y1:
        face_region = frame[y1:face_y2, x1:x2]
        if face_region.size > 0:
            blurred_face = cv2.GaussianBlur(face_region, (15, 15), 0)
            frame[y1:face_y2, x1:x2] = blurred_face
    return frame

# --- BI·∫æN L∆ØU TR·ªÆ ---
last_custom_results = [] 
frame_count = 0

# --- KH·ªûI T·∫†O H·ªÜ TH·ªêNG √ÇM THANH ---
print("üîä ƒêang kh·ªüi t·∫°o h·ªá th·ªëng √¢m thanh...")
audio_queue = setup_audio_system()
print("‚úÖ H·ªá th·ªëng √¢m thanh ƒë√£ s·∫µn s√†ng")

cap = cv2.VideoCapture(0 if USE_CAM else VIDEO_PATH)
orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Ng∆∞·ª°ng Y ƒë·ªÉ x√©t G·∫ßn/Xa (C·∫°nh d∆∞·ªõi khung h√¨nh > 75% chi·ªÅu cao ·∫£nh l√† R·∫•t G·∫ßn)
danger_line = int(orig_h * 0.75)

# C·∫•u h√¨nh k√≠ch th∆∞·ªõc trung b√¨nh cho c√°c lo·∫°i v·∫≠t th·ªÉ (ƒë·ªÉ t√≠nh kho·∫£ng c√°ch)
class_avg_sizes = {
    "person": {"width_ratio": 2.5},
    "car": {"width_ratio": 0.37},
    "bicycle": {"width_ratio": 2.3},
    "motorcycle": {"width_ratio": 2.4},
    "bus": {"width_ratio": 0.3},
    "traffic light": {"width_ratio": 2.95},
    "stop sign": {"width_ratio": 2.55},
    "bench": {"width_ratio": 1.6},
    "cat": {"width_ratio": 1.9},
    "dog": {"width_ratio": 1.5},
}

# Bi·∫øn global cho h·ªá th·ªëng √¢m thanh
audio_queue = None
audio_thread = None

def setup_audio_system():
    """Kh·ªüi t·∫°o h·ªá th·ªëng ph√°t √¢m thanh"""
    global audio_queue, audio_thread
    
    def speak(q):
        engine = pyttsx3.init()
        engine.setProperty('rate', 235)  # T·ªëc ƒë·ªô n√≥i
        engine.setProperty('volume', 1.0)  # √Çm l∆∞·ª£ng

        while True:
            if not q.empty():
                label, distance, position = q.get()
                rounded_distance = round(distance * 2) / 2  # L√†m tr√≤n 0.5
                
                # Chuy·ªÉn ƒë·ªïi s·ªë th√†nh chu·ªói (b·ªè .0 n·∫øu l√† s·ªë nguy√™n)
                rounded_distance_str = str(int(rounded_distance)) if rounded_distance.is_integer() else str(rounded_distance)
                
                # Ph√°t √¢m thanh th√¥ng b√°o
                if label in class_avg_sizes:
                    engine.say(f"{label} IS {rounded_distance_str} METERS ON {position}")
                    engine.runAndWait()
                
                # X√≥a queue sau khi ph√°t
                with q.mutex:
                    q.queue.clear()
            else:
                time.sleep(0.1)  # Tr√°nh busy waiting
    
    # T·∫°o queue v√† thread cho audio
    audio_queue = Queue()
    audio_thread = Thread(target=speak, args=(audio_queue,))
    audio_thread.daemon = True  # Thread s·∫Ω d·ª´ng khi main program d·ª´ng
    audio_thread.start()
    
    return audio_queue

def calculate_distance(object_width, frame_width, label):
    """
    T√≠nh kho·∫£ng c√°ch d·ª±a tr√™n ƒë·ªô r·ªông v·∫≠t th·ªÉ trong frame
    """
    # √Åp d·ª•ng h·ªá s·ªë hi·ªáu ch·ªânh cho t·ª´ng lo·∫°i v·∫≠t th·ªÉ
    if label in class_avg_sizes:
        object_width *= class_avg_sizes[label]["width_ratio"]
    
    # C√¥ng th·ª©c t√≠nh kho·∫£ng c√°ch d·ª±a tr√™n FOV 70 ƒë·ªô
    distance = (frame_width * 0.5) / np.tan(np.radians(70 / 2)) / (object_width + 1e-6)
    return round(distance, 2)

def get_position(frame_width, x_center):
    """X√°c ƒë·ªãnh v·ªã tr√≠ c·ªßa v·∫≠t th·ªÉ (LEFT/FORWARD/RIGHT)"""
    if x_center < frame_width // 3:
        return "LEFT"
    elif x_center < 2 * (frame_width // 3):
        return "FORWARD"
    else:
        return "RIGHT"

def blur_person_face(frame, x1, y1, x2, y2):
    """L√†m m·ªù khu√¥n m·∫∑t ng∆∞·ªùi ƒë·ªÉ b·∫£o v·ªá privacy"""
    h = y2 - y1
    # L√†m m·ªù ph·∫ßn ƒë·∫ßu (8% chi·ªÅu cao t·ª´ tr√™n xu·ªëng)
    face_y2 = y1 + int(0.08 * h)
    if face_y2 > y1:
        face_region = frame[y1:face_y2, x1:x2]
        if face_region.size > 0:
            blurred_face = cv2.GaussianBlur(face_region, (15, 15), 0)
            frame[y1:face_y2, x1:x2] = blurred_face
    return frame

# Bi·∫øn l∆∞u tr·ªØ v·∫≠t th·ªÉ g·∫ßn nh·∫•t ƒë·ªÉ ph√°t √¢m thanh
nearest_object_info = {"label": None, "distance": float('inf'), "last_announcement": 0}

while True:
    ret, frame = cap.read()
    if not ret: break
    
    frame_count += 1
    start_time = time.time()

    # --- B∆Ø·ªöC 3: TI·ªÄN X·ª¨ L√ù (T·ªêI ∆ØU REAL-TIME) ---
    # 1. Resize tr∆∞·ªõc ƒë·ªÉ gi·∫£m t·∫£i cho c√°c b∆∞·ªõc sau
    img_640 = cv2.resize(frame, (640, 640), interpolation=cv2.INTER_CUBIC)
    
    # 2. √Åp d·ª•ng LUT thay cho convertScaleAbs (X·ª≠ l√Ω adaptive s√°ng/t·ªëi)
    img_640 = cv2.LUT(img_640, lut_table)
    
    # 3. Chuy·ªÉn ƒë·ªïi format cho AI
    blob = img_640.astype(np.float32) / 255.0
    blob = np.transpose(blob, (2, 0, 1))
    blob = np.expand_dims(blob, axis=0)

    # --- B∆Ø·ªöC 4: CH·∫†Y INFERENCE ---
    # Model COCO: Ch·∫°y 100% frame
    results_coco = session_coco.run(None, {session_coco.get_inputs()[0].name: blob})[0][0]
    
    # Model Custom: Skip frame (Ch·∫°y frame ch·∫µn, hi·ªÉn th·ªã k·∫øt qu·∫£ c≈© ·ªü frame l·∫ª)
    if session_custom is not None:
        if frame_count % 2 == 0:
            last_custom_results = session_custom.run(None, {session_custom.get_inputs()[0].name: blob})[0][0]

    # --- B∆Ø·ªöC 5: V·∫º K·∫æT QU·∫¢ V√Ä T√çNH KHO·∫¢NG C√ÅCH ---
    nearest_object = None
    min_distance = float('inf')
    current_time = time.time()
    
    # V·∫Ω COCO v·ªõi t√≠nh kho·∫£ng c√°ch
    for pred in results_coco:
        x1, y1, x2, y2, score, cls_id = pred
        if score > 0.35:
            ix1, iy1 = int(x1 * orig_w / 640), int(y1 * orig_h / 640)
            ix2, iy2 = int(x2 * orig_w / 640), int(y2 * orig_h / 640)
            
            label = class_names_coco[int(cls_id)]
            
            # T√≠nh kho·∫£ng c√°ch n·∫øu v·∫≠t th·ªÉ trong danh s√°ch h·ªó tr·ª£
            if label in class_avg_sizes:
                object_width = ix2 - ix1
                distance = calculate_distance(object_width, orig_w, label)
                
                # T√¨m v·∫≠t th·ªÉ g·∫ßn nh·∫•t
                if distance < min_distance:
                    min_distance = distance
                    x_center = (ix1 + ix2) // 2
                    nearest_object = (label, distance, x_center)
                
                # L√†m m·ªù khu√¥n m·∫∑t n·∫øu l√† ng∆∞·ªùi
                if label == "person":
                    frame = blur_person_face(frame, ix1, iy1, ix2, iy2)
                
                # Ch·ªçn m√†u d·ª±a tr√™n kho·∫£ng c√°ch thay v√¨ v·ªã tr√≠ Y
                if distance <= 3.0:  # R·∫•t g·∫ßn - ƒê·ªè
                    color = (0, 0, 255)
                elif distance <= 8.0:  # G·∫ßn - Cam
                    color = (0, 165, 255)
                else:  # Xa - Xanh l√°
                    color = (0, 255, 0)
                
                display_label = f"{label} - {distance:.1f}m"
            else:
                # S·ª≠ d·ª•ng logic c≈© cho v·∫≠t th·ªÉ kh√¥ng h·ªó tr·ª£ t√≠nh kho·∫£ng c√°ch
                color = (0, 0, 255) if iy2 > danger_line else (0, 255, 0)
                display_label = f"{label} {'(NEAR)' if iy2 > danger_line else ''}"
            
            cv2.rectangle(frame, (ix1, iy1), (ix2, iy2), color, 2)
            cv2.putText(frame, display_label, (ix1, iy1-10), 0, 0.5, color, 2)

    # V·∫Ω Custom (M√†u V√†ng - Ch·ªëng ch·ªõp t·∫Øt)
    for pred in last_custom_results:
        x1, y1, x2, y2, score, cls_id = pred
        if score > 0.25:
            ix1, iy1 = int(x1 * orig_w / 640), int(y1 * orig_h / 640)
            ix2, iy2 = int(x2 * orig_w / 640), int(y2 * orig_h / 640)
            
            # C·∫£nh b√°o g·∫ßn cho v·∫≠t th·ªÉ Custom
            color = (0, 0, 255) if iy2 > danger_line else (0, 255, 255)
            
            cv2.rectangle(frame, (ix1, iy1), (ix2, iy2), color, 2)
            cv2.putText(frame, f"TARGET: {class_names_custom[int(cls_id)]}", (ix1, iy1-10), 0, 0.5, color, 2)

    # X·ª≠ l√Ω √¢m thanh cho v·∫≠t th·ªÉ g·∫ßn nh·∫•t
    if nearest_object and nearest_object[1] <= 12.5:  # Ch·ªâ th√¥ng b√°o v·∫≠t th·ªÉ ‚â§ 12.5m
        # Tr√°nh spam √¢m thanh - ch·ªâ ph√°t m·ªói 3 gi√¢y
        if (nearest_object_info["label"] != nearest_object[0] or 
            abs(nearest_object_info["distance"] - nearest_object[1]) > 1.0 or
            current_time - nearest_object_info["last_announcement"] > 3.0):
            
            position = get_position(orig_w, nearest_object[2])
            audio_queue.put((nearest_object[0], nearest_object[1], position))
            
            nearest_object_info["label"] = nearest_object[0]
            nearest_object_info["distance"] = nearest_object[1]
            nearest_object_info["last_announcement"] = current_time

    # Hi·ªÉn th·ªã FPS v√† th√¥ng tin
    fps = 1.0 / (time.time() - start_time)
    cv2.putText(frame, f"FPS: {fps:.1f} | CPU Mode", (20, 40), 0, 0.7, (255, 255, 255), 2)
    
    cv2.imshow("DIPR Project - Blind Support", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'): break

cap.release()
cv2.destroyAllWindows()