import cv2
import numpy as np
import onnxruntime as ort
import time
import os
import pyttsx3
from threading import Thread
from queue import Queue

# --- 1. C·∫§U H√åNH ---
USE_CAM = True  # Chuy·ªÉn sang True ƒë·ªÉ s·ª≠ d·ª•ng camera
VIDEO_PATH = "./sample_video.mp4"  # ƒê∆∞·ªùng d·∫´n video m·∫´u (n·∫øu c√≥)

# --- 2. KH·ªûI T·∫†O B·∫¢NG LUT (GAMMA CORRECTION) ---
gamma = 0.8
invGamma = 1.0 / gamma
lut_table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype("uint8")

# --- 3. KH·ªûI T·∫†O DUAL MODEL ---
providers = ['CPUExecutionProvider']
session_coco = None
session_custom = None

try:
    opt = ort.SessionOptions()
    # Load COCO model (80 classes)
    session_coco = ort.InferenceSession("yolov10s.onnx", providers=providers, sess_options=opt)
    print("‚úÖ COCO Model (yolov10s.onnx) loaded successfully")
    
    # Load Custom model (door, trash_can)
    session_custom = ort.InferenceSession("best.onnx", providers=providers, sess_options=opt)
    print("‚úÖ Custom Model (best.onnx) loaded successfully")
    
    print("üöÄ DUAL MODEL SYSTEM READY!")
except Exception as e:
    print(f"‚ùå L·ªói loading models: {e}"); exit()

# --- 4. DANH S√ÅCH CLASSES ---
class_names_coco = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']

# Custom model classes (t·ª´ best.onnx)
class_names_custom = ['door', 'trash_can']

# --- 5. H·ªÜ TH·ªêNG √ÇM THANH V√Ä T√çNH KHO·∫¢NG C√ÅCH ---

# C·∫•u h√¨nh k√≠ch th∆∞·ªõc cho t√≠nh kho·∫£ng c√°ch - T·∫§T C·∫¢ OBJECT COCO + CUSTOM
class_avg_sizes = {
    # CUSTOM OBJECTS - ƒë∆∞·ª£c th√™m v√†o
    "door": {"width_ratio": 0.8},      # C·ª≠a
    "trash_can": {"width_ratio": 2.5}, # Th√πng r√°c
    
    # Con ng∆∞·ªùi v√† ƒë·ªông v·∫≠t
    "person": {"width_ratio": 2.5},
    "bird": {"width_ratio": 8.0},
    "cat": {"width_ratio": 1.9},
    "dog": {"width_ratio": 1.5},
    "horse": {"width_ratio": 0.8},
    "sheep": {"width_ratio": 1.2},
    "cow": {"width_ratio": 0.6},
    "elephant": {"width_ratio": 0.3},
    "bear": {"width_ratio": 0.9},
    "zebra": {"width_ratio": 0.8},
    "giraffe": {"width_ratio": 0.4},
    
    # Ph∆∞∆°ng ti·ªán
    "bicycle": {"width_ratio": 2.3},
    "car": {"width_ratio": 0.37},
    "motorcycle": {"width_ratio": 2.4},
    "airplane": {"width_ratio": 0.1},
    "bus": {"width_ratio": 0.3},
    "train": {"width_ratio": 0.2},
    "truck": {"width_ratio": 0.25},
    "boat": {"width_ratio": 0.5},
    
    # Giao th√¥ng
    "traffic light": {"width_ratio": 2.95},
    "fire hydrant": {"width_ratio": 3.0},
    "stop sign": {"width_ratio": 2.55},
    "parking meter": {"width_ratio": 4.0},
    
    # ƒê·ªì v·∫≠t l·ªõn
    "bench": {"width_ratio": 1.6},
    "chair": {"width_ratio": 2.2},
    "couch": {"width_ratio": 1.0},
    "bed": {"width_ratio": 0.8},
    "dining table": {"width_ratio": 1.2},
    "toilet": {"width_ratio": 2.8},
    "tv": {"width_ratio": 1.8},
    
    # ƒê·ªì v·∫≠t v·ª´a
    "backpack": {"width_ratio": 3.5},
    "umbrella": {"width_ratio": 2.8},
    "handbag": {"width_ratio": 4.0},
    "tie": {"width_ratio": 8.0},
    "suitcase": {"width_ratio": 2.0},
    "laptop": {"width_ratio": 3.2},
    "book": {"width_ratio": 6.0},
    "clock": {"width_ratio": 3.0},
    "vase": {"width_ratio": 3.5},
    
    # ƒê·ªì th·ªÉ thao
    "frisbee": {"width_ratio": 4.5},
    "skis": {"width_ratio": 6.0},
    "snowboard": {"width_ratio": 3.5},
    "sports ball": {"width_ratio": 5.5},
    "kite": {"width_ratio": 2.5},
    "baseball bat": {"width_ratio": 4.8},
    "baseball glove": {"width_ratio": 4.2},
    "skateboard": {"width_ratio": 3.8},
    "surfboard": {"width_ratio": 2.8},
    "tennis racket": {"width_ratio": 3.2},
    
    # ƒê·ªì ƒÉn u·ªëng (ƒë·ªì v·∫≠t nh·ªè)
    "bottle": {"width_ratio": 8.0},
    "wine glass": {"width_ratio": 8.5},
    "cup": {"width_ratio": 9.0},
    "fork": {"width_ratio": 12.0},
    "knife": {"width_ratio": 10.0},
    "spoon": {"width_ratio": 11.0},
    "bowl": {"width_ratio": 6.0},
    "banana": {"width_ratio": 10.0},
    "apple": {"width_ratio": 8.5},
    "sandwich": {"width_ratio": 5.0},
    "orange": {"width_ratio": 9.0},
    "broccoli": {"width_ratio": 7.0},
    "carrot": {"width_ratio": 12.0},
    "hot dog": {"width_ratio": 8.0},
    "pizza": {"width_ratio": 4.0},
    "donut": {"width_ratio": 9.0},
    "cake": {"width_ratio": 4.5},
    
    # ƒê·ªì gia d·ª•ng
    "potted plant": {"width_ratio": 3.0},
    "mouse": {"width_ratio": 10.0},
    "remote": {"width_ratio": 7.0},
    "keyboard": {"width_ratio": 3.8},
    "cell phone": {"width_ratio": 8.0},
    "microwave": {"width_ratio": 1.8},
    "oven": {"width_ratio": 1.5},
    "toaster": {"width_ratio": 3.5},
    "sink": {"width_ratio": 1.4},
    "refrigerator": {"width_ratio": 0.9},
    "scissors": {"width_ratio": 8.0},
    "teddy bear": {"width_ratio": 3.2},
    "hair drier": {"width_ratio": 5.0},
    "toothbrush": {"width_ratio": 12.0},
}

def setup_audio_system():
    """Kh·ªüi t·∫°o h·ªá th·ªëng ph√°t √¢m thanh v·ªõi ki·ªÉm so√°t gi√°n ƒëo·∫°n"""
    def speak(q):
        global audio_is_speaking
        try:
            engine = pyttsx3.init()
            engine.setProperty('rate', 100)  # Ch·∫≠m h∆°n ƒë·ªÉ nghe r√µ
            engine.setProperty('volume', 1.0)

            while True:
                if not q.empty() and not audio_is_speaking:
                    audio_is_speaking = True
                    
                    # L·∫•y t·∫•t c·∫£ objects trong queue ƒë·ªÉ t·∫°o th√¥ng b√°o t·ªïng h·ª£p
                    objects_by_position = {"LEFT": [], "FORWARD": [], "RIGHT": []}
                    
                    # Thu th·∫≠p t·∫•t c·∫£ objects
                    while not q.empty():
                        try:
                            label, distance, position = q.get_nowait()
                            objects_by_position[position].append(label)
                        except:
                            break
                    
                    # T·∫°o th√¥ng b√°o theo v·ªã tr√≠
                    message_parts = []
                    
                    for position in ["LEFT", "FORWARD", "RIGHT"]:
                        if objects_by_position[position]:
                            objects_list = ", ".join(objects_by_position[position])
                            message_parts.append(f"{objects_list} on {position}")
                    
                    # Ph√°t th√¥ng b√°o t·ªïng h·ª£p
                    if message_parts:
                        full_message = ". ".join(message_parts)
                        print(f"üîä Speaking: {full_message}")  # Debug
                        engine.say(full_message)
                        engine.runAndWait()
                        print(f"‚úÖ Audio completed")  # Debug
                        time.sleep(1.0)  # Pause sau khi ph√°t xong
                    
                    audio_is_speaking = False
                
                else:
                    time.sleep(0.1)  # Ki·ªÉm tra th∆∞·ªùng xuy√™n h∆°n
                    
        except Exception as e:
            print(f"‚ùå Audio error: {e}")
            audio_is_speaking = False
    
    audio_queue = Queue()
    audio_thread = Thread(target=speak, args=(audio_queue,))
    audio_thread.daemon = True
    audio_thread.start()
    
    return audio_queue

def calculate_distance(object_width, frame_width, label):
    """T√≠nh kho·∫£ng c√°ch d·ª±a tr√™n ƒë·ªô r·ªông v·∫≠t th·ªÉ trong frame - H·ªó tr·ª£ T·∫§T C·∫¢ object"""
    # √Åp d·ª•ng h·ªá s·ªë hi·ªáu ch·ªânh n·∫øu c√≥ trong danh s√°ch
    if label in class_avg_sizes:
        object_width *= class_avg_sizes[label]["width_ratio"]
    else:
        # H·ªá s·ªë m·∫∑c ƒë·ªãnh cho object kh√¥ng c√≥ trong danh s√°ch
        object_width *= 2.0  # H·ªá s·ªë trung b√¨nh
    
    # C√¥ng th·ª©c t√≠nh kho·∫£ng c√°ch d·ª±a tr√™n FOV 70 ƒë·ªô
    distance = (frame_width * 0.5) / np.tan(np.radians(70 / 2)) / (object_width + 1e-6)
    return round(distance, 2)

def get_position(frame_width, x_center):
    """X√°c ƒë·ªãnh v·ªã tr√≠ c·ªßa v·∫≠t th·ªÉ (LEFT/FORWARD/RIGHT)"""
    if x_center < frame_width // 3:
        return "LEFT"
    elif x_center < 2 * (frame_width // 3):
        return "FORWARD"
    else:
        return "RIGHT"

def blur_person_face(frame, x1, y1, x2, y2):
    """L√†m m·ªù khu√¥n m·∫∑t ng∆∞·ªùi ƒë·ªÉ b·∫£o v·ªá privacy"""
    h = y2 - y1
    face_y2 = y1 + int(0.08 * h)
    if face_y2 > y1:
        face_region = frame[y1:face_y2, x1:x2]
        if face_region.size > 0:
            blurred_face = cv2.GaussianBlur(face_region, (15, 15), 0)
            frame[y1:face_y2, x1:x2] = blurred_face
    return frame

def validate_trash_can(ix1, iy1, ix2, iy2, orig_h, orig_w):
    """X√°c th·ª±c trash_can ƒë·ªÉ gi·∫£m false positive"""
    box_height = iy2 - iy1
    box_width = ix2 - ix1
    box_center_y = (iy1 + iy2) // 2
    box_area = box_width * box_height
    frame_area = orig_w * orig_h
    
    # 1. Th√πng r√°c th∆∞·ªùng ·ªü ph·∫ßn d∆∞·ªõi c·ªßa frame (kh√¥ng ·ªü tr√™n cao)
    if box_center_y < orig_h * 0.3:  # Lo·∫°i b·ªè v·∫≠t th·ªÉ ·ªü 30% tr√™n c√πng
        print(f"‚ùå TRASH_CAN rejected: Too high (center_y={box_center_y} < {orig_h * 0.3:.0f})")
        return False
    
    # 2. Th√πng r√°c c√≥ t·ª∑ l·ªá height/width h·ª£p l√Ω 
    aspect_ratio = box_height / (box_width + 1e-6)
    if aspect_ratio < 0.5 or aspect_ratio > 3.0:
        print(f"‚ùå TRASH_CAN rejected: Bad aspect ratio ({aspect_ratio:.2f})")
        return False
    
    # 3. K√≠ch th∆∞·ªõc kh√¥ng ƒë∆∞·ª£c qu√° l·ªõn (lo·∫°i b·ªè detection to√†n m√†n h√¨nh)
    area_ratio = box_area / frame_area
    if area_ratio > 0.7:  # Kh√¥ng ƒë∆∞·ª£c chi·∫øm qu√° 70% m√†n h√¨nh
        print(f"‚ùå TRASH_CAN rejected: Too large ({area_ratio:.2f} of frame)")
        return False
    
    # 4. K√≠ch th∆∞·ªõc t·ªëi thi·ªÉu
    min_area = frame_area * 0.001  # √çt nh·∫•t 0.1% di·ªán t√≠ch frame
    if box_area < min_area:
        print(f"‚ùå TRASH_CAN rejected: Too small ({box_area} < {min_area:.0f})")
        return False
    
    print(f"‚úÖ TRASH_CAN validated: area_ratio={area_ratio:.3f}, aspect_ratio={aspect_ratio:.2f}")
    return True

# Bi·∫øn global ƒë·ªÉ ki·ªÉm so√°t audio
audio_is_speaking = False

# --- 6. KH·ªûI T·∫†O ---
print("üîä ƒêang kh·ªüi t·∫°o h·ªá th·ªëng √¢m thanh...")
audio_queue = setup_audio_system()
print("‚úÖ H·ªá th·ªëng √¢m thanh ƒë√£ s·∫µn s√†ng")

# Kh·ªüi t·∫°o camera v·ªõi ∆∞u ti√™n DroidCam
def init_camera():
    """Kh·ªüi t·∫°o camera v·ªõi ∆∞u ti√™n DroidCam, fallback v·ªÅ webcam"""
    if USE_CAM:
        # Danh s√°ch IP th∆∞·ªùng d√πng cho DroidCam (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh theo m·∫°ng c·ªßa b·∫°n)
        droidcam_urls = [
            "http://192.168.1.100:4747/video",  # IP m·∫∑c ƒë·ªãnh DroidCam
            "http://192.168.43.1:4747/video",   # Hotspot Android
            "http://10.0.0.100:4747/video",     # M·∫°ng kh√°c
        ]
        
        print("üì± ƒêang t√¨m ki·∫øm DroidCam...")
        
        # Th·ª≠ k·∫øt n·ªëi v·ªõi t·ª´ng URL DroidCam
        for url in droidcam_urls:
            print(f"üîç Th·ª≠ k·∫øt n·ªëi: {url}")
            try:
                cap = cv2.VideoCapture(url)
                # Th·ª≠ ƒë·ªçc m·ªôt frame ƒë·ªÉ ki·ªÉm tra k·∫øt n·ªëi
                ret, frame = cap.read()
                if ret and frame is not None:
                    print(f"‚úÖ DroidCam k·∫øt n·ªëi th√†nh c√¥ng: {url}")
                    return cap
                else:
                    cap.release()
            except Exception as e:
                print(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi {url}: {e}")
        
        # N·∫øu kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c DroidCam, fallback v·ªÅ webcam
        print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y DroidCam, chuy·ªÉn sang webcam m·∫∑c ƒë·ªãnh...")
        try:
            cap = cv2.VideoCapture(0)
            ret, frame = cap.read()
            if ret and frame is not None:
                print("‚úÖ Webcam m·∫∑c ƒë·ªãnh k·∫øt n·ªëi th√†nh c√¥ng")
                return cap
            else:
                cap.release()
                raise Exception("Webcam kh√¥ng ho·∫°t ƒë·ªông")
        except Exception as e:
            print(f"‚ùå L·ªói k·∫øt n·ªëi webcam: {e}")
            raise e
    else:
        # S·ª≠ d·ª•ng video file
        print(f"üìπ ƒêang m·ªü video file: {VIDEO_PATH}")
        return cv2.VideoCapture(VIDEO_PATH)

cap = init_camera()
orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Ng∆∞·ª°ng Y ƒë·ªÉ x√©t G·∫ßn/Xa (cho v·∫≠t th·ªÉ kh√¥ng h·ªó tr·ª£ t√≠nh kho·∫£ng c√°ch)
danger_line = int(orig_h * 0.75)

# Bi·∫øn l∆∞u tr·ªØ ƒë·ªÉ tr√°nh spam √¢m thanh
nearest_object_info = {"label": None, "distance": float('inf'), "last_announcement": 0}
frame_count = 0

# L∆∞u k·∫øt qu·∫£ custom model ƒë·ªÉ t·ªëi ∆∞u performance
last_custom_results = []

# --- 7. MAIN LOOP ---
while True:
    ret, frame = cap.read()
    if not ret: 
        break
    
    frame_count += 1
    start_time = time.time()

    # Ti·ªÅn x·ª≠ l√Ω v·ªõi Letterbox (gi·ªëng model-tranied.py)
    r = min(640 / orig_w, 640 / orig_h)
    new_unpad = (int(round(orig_w * r)), int(round(orig_h * r)))
    img_resized = cv2.resize(frame, new_unpad, interpolation=cv2.INTER_LINEAR)
    
    img_640 = np.full((640, 640, 3), 114, dtype=np.uint8)
    dw = (640 - new_unpad[0]) // 2
    dh = (640 - new_unpad[1]) // 2
    img_640[dh:dh + new_unpad[1], dw:dw + new_unpad[0]] = img_resized
    
    img_640 = cv2.LUT(img_640, lut_table)
    blob = img_640.astype(np.float32) / 255.0
    blob = np.transpose(blob, (2, 0, 1))
    blob = np.expand_dims(blob, axis=0)

    # DUAL MODEL INFERENCE
    # COCO Model - ch·∫°y m·ªói frame
    results_coco = session_coco.run(None, {session_coco.get_inputs()[0].name: blob})[0][0]
    
    # Custom Model - ch·ªâ ch·∫°y m·ªói 2 frame ƒë·ªÉ t·ªëi ∆∞u performance
    if frame_count % 2 == 0:
        last_custom_results = session_custom.run(None, {session_custom.get_inputs()[0].name: blob})[0][0]

    # H√†m mapping t·ªça ƒë·ªô (t·ª´ letterbox v·ªÅ original)
    def scale_coords(x1, y1, x2, y2):
        rx1 = int((x1 - dw) / r)
        ry1 = int((y1 - dh) / r)
        rx2 = int((x2 - dw) / r)
        ry2 = int((y2 - dh) / r)
        return (max(0, rx1), max(0, ry1), min(orig_w, rx2), min(orig_h, ry2))

    # X·ª≠ l√Ω k·∫øt qu·∫£ DUAL MODEL v·ªõi t√≠nh kho·∫£ng c√°ch v√† √¢m thanh
    objects_under_2m = []  # L∆∞u t·∫•t c·∫£ v·∫≠t th·ªÉ d∆∞·ªõi 2m
    current_time = time.time()
    
    # === X·ª¨ L√ù COCO MODEL RESULTS ===
    for pred in results_coco:
        x1, y1, x2, y2, score, cls_id = pred
        if score > 0.35:
            ix1, iy1, ix2, iy2 = scale_coords(x1, y1, x2, y2)
            
            label = class_names_coco[int(cls_id)]
            
            # T·∫§T C·∫¢ OBJECT ƒê·ªÄU ƒê∆Ø·ª¢C T√çNH KHO·∫¢NG C√ÅCH
            object_width = ix2 - ix1
            distance = calculate_distance(object_width, orig_w, label)
            
            # Thu th·∫≠p v·∫≠t th·ªÉ d∆∞·ªõi 2m cho audio
            if distance < 2.0:
                x_center = (ix1 + ix2) // 2
                position = get_position(orig_w, x_center)
                objects_under_2m.append((label, distance, position))
            
            # L√†m m·ªù khu√¥n m·∫∑t n·∫øu l√† ng∆∞·ªùi
            if label == "person":
                frame = blur_person_face(frame, ix1, iy1, ix2, iy2)
            
            # NG∆Ø·ª†NG M√ÄU: >= 2m = XANH, < 2m = ƒê·ªé
            color = (0, 255, 0) if distance >= 2.0 else (0, 0, 255)
            
            # Hi·ªÉn th·ªã label v·ªõi kho·∫£ng c√°ch
            display_label = f"{label} - {distance:.1f}m"
            
            cv2.rectangle(frame, (ix1, iy1), (ix2, iy2), color, 2)
            cv2.putText(frame, display_label, (ix1, iy1-10), 0, 0.5, color, 2)

    # === X·ª¨ L√ù CUSTOM MODEL RESULTS ===
    for pred in last_custom_results:
        x1, y1, x2, y2, score, cls_id = pred
        label = class_names_custom[int(cls_id)]
        
        if score > (0.4 if label == 'door' else 0.45):  # TƒÉng t·ª´ 0.25 l√™n 0.45 cho trash_can
            ix1, iy1, ix2, iy2 = scale_coords(x1, y1, x2, y2)
            
            # Validation ƒë·∫∑c bi·ªát cho trash_can
            if label == 'trash_can' and not validate_trash_can(ix1, iy1, ix2, iy2, orig_h, orig_w):
                continue  # B·ªè qua detection n√†y n·∫øu kh√¥ng h·ª£p l·ªá
            
            # X√°c th·ª±c trash_can ƒë·ªÉ gi·∫£m false positive
            if label == "trash_can" and not validate_trash_can(ix1, iy1, ix2, iy2, orig_h, orig_w):
                continue
            
            # Debug info cho custom detections
            print(f"üìç {label.upper()}: score={score:.3f}, pos=({ix1},{iy1},{ix2},{iy2}), center_y={(iy1+iy2)//2}")
            
            # T√≠nh kho·∫£ng c√°ch cho custom objects
            object_width = ix2 - ix1
            distance = calculate_distance(object_width, orig_w, label)
            
            # Thu th·∫≠p v·∫≠t th·ªÉ d∆∞·ªõi 2m cho audio
            if distance < 2.0:
                x_center = (ix1 + ix2) // 2
                position = get_position(orig_w, x_center)
                objects_under_2m.append((f"TARGET: {label.upper()}", distance, position))
            
            # M√†u ƒë·∫∑c bi·ªát cho custom objects: >= 2m = V√ÄNG, < 2m = ƒê·ªé
            color = (0, 255, 255) if distance >= 2.0 else (0, 0, 255)
            
            display_label = f"TARGET: {label.upper()} - {distance:.1f}m ({score:.2f})"
            
            cv2.rectangle(frame, (ix1, iy1), (ix2, iy2), color, 2)
            cv2.putText(frame, display_label, (ix1, iy1-10), 0, 0.5, color, 2)

    # X·ª≠ l√Ω √¢m thanh cho T·∫§T C·∫¢ v·∫≠t th·ªÉ d∆∞·ªõi 2m v·ªõi ki·ªÉm so√°t gi√°n ƒëo·∫°n
    if objects_under_2m and not audio_is_speaking and (current_time - nearest_object_info["last_announcement"] > 5.0):
        # G·ª≠i t·∫•t c·∫£ objects d∆∞·ªõi 2m v√†o queue
        for label, distance, position in objects_under_2m:
            audio_queue.put((label, distance, position))
        
        nearest_object_info["last_announcement"] = current_time
        print(f"üîä Queuing {len(objects_under_2m)} objects under 2m to audio system")  # Debug

    # Hi·ªÉn th·ªã FPS v√† th√¥ng tin h·ªá th·ªëng
    fps = 1.0 / (time.time() - start_time)
    info_text = f"FPS: {fps:.1f} | DUAL MODEL: COCO + CUSTOM | Distance + Audio"
    cv2.putText(frame, info_text, (20, 40), 0, 0.7, (255, 255, 255), 2)
    
    # Hi·ªÉn th·ªã th·ªëng k√™
    stats_text = f"Objects < 2m: {len(objects_under_2m)} | Audio: {'SPEAKING' if audio_is_speaking else 'READY'}"
    cv2.putText(frame, stats_text, (20, 70), 0, 0.5, (255, 255, 255), 2)
    
    cv2.imshow("DIPR Project - DUAL MODEL: Object Detection + Distance + Audio", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'): 
        break

cap.release()
cv2.destroyAllWindows()
